{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import treelite\n",
    "import tl2cgen\n",
    "import sklearn.datasets as datasets\n",
    "import xgboost as xgb\n",
    "import numpy as np\n",
    "import subprocess\n",
    "import random\n",
    "import subprocess\n",
    "import matplotlib.pyplot as plt\n",
    "import platform\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from ucimlrepo import fetch_ucirepo "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = [fetch_ucirepo(id=17), fetch_ucirepo(id=31), fetch_ucirepo(id=545), fetch_ucirepo(id=109), fetch_ucirepo(id=159), fetch_ucirepo(id=94), fetch_ucirepo(id=42), fetch_ucirepo(id=572)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets_names = ['Breast Cancer', 'Covertype', 'Rice', 'Wine', 'Magic','Spam', 'Glass', 'Bankruptcy']\n",
    "tree_depths = [3, 5, 10, 15, 20]\n",
    "nr_root_nodes = 20\n",
    "nr_validation_reuses = 1000\n",
    "nr_instances_validation = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def FreeTayK(path: str):\n",
    "    with open(path, 'r+') as f:\n",
    "        data = f.readlines()\n",
    "        f.seek(0, 0)\n",
    "        f.write('#include \"quantize.c\"\\n' + ''.join(data[2:]))\n",
    "    \n",
    "def create_c_array_string(array):\n",
    "    array_string = \"{\\n\"\n",
    "    array_string += \",\\n\".join(\"{\" + \", \".join(str(x) for x in row) + \"}\" for row in array)\n",
    "    array_string += \"\\n}\"\n",
    "    return array_string\n",
    "\n",
    "def count_decimals(number):\n",
    "    str_num = str(number)\n",
    "    if '.' not in str_num:\n",
    "        return 0\n",
    "    return len(str_num) - str_num.index('.') - 1\n",
    "\n",
    "def beg_c_file_str(nr_datasets, nr_treedepths) :\n",
    "    a = \"\"\"\n",
    "#include <time.h>\n",
    "#include <stdio.h>\n",
    "#include <stdlib.h>\n",
    "#include <string.h>\n",
    "#include <windows.h>\n",
    "#include \"dll_paths.h\"\n",
    "\n",
    "\n",
    "typedef void (*PredictFunction)(union Entry*, int, float*);\n",
    "int main() {\n",
    "char times[\"\"\"+str(nr_datasets*nr_treedepths*30)+\"\"\"] = \"\";\n",
    "int k=0;\n",
    "int nr_treedepths = \"\"\"+str(nr_treedepths)+\"\"\";\n",
    "    \"\"\"\n",
    "    return a\n",
    "\n",
    "def mid_c_file_str(i, nr_results, reuses):\n",
    "    a = \"\"\"\n",
    "//////////////////////////////////////////////\n",
    "\n",
    "    for (int i = (0+(k*3))*nr_treedepths; i < (3 +(k*3))*nr_treedepths; i++) {\n",
    "        HINSTANCE hDLL = LoadLibraryA(dll_paths[i]);\n",
    "        if (hDLL == NULL) {\n",
    "            return 1;\n",
    "        }\n",
    "        PredictFunction predict = (PredictFunction) GetProcAddress(hDLL, \"predict\");\n",
    "        clock_t start = clock();\n",
    "        for (int z = 0; z < \"\"\"+str(reuses)+\"\"\"; z++) {\n",
    "            for (int j = 0; j < sizeof(validation_instances\"\"\"+str(i)+\"\"\")/sizeof(validation_instances\"\"\"+str(i)+\"\"\"[0]); j++) {\n",
    "                float result[\"\"\"+str(nr_results)+\"\"\"]={0.0f};\n",
    "                predict(validation_instances\"\"\"+str(i)+\"\"\"[j], 0, &result);\n",
    "            }\n",
    "        }\n",
    "        FreeLibrary(hDLL);\n",
    "        clock_t end = clock();\n",
    "        double elapsed_time = ((double) (end - start)) / CLOCKS_PER_SEC;\n",
    "        sprintf(times + strlen(times), \"%6.3f\\\\n\", elapsed_time);\n",
    "    }\n",
    "k++;\n",
    "\n",
    "//////////////////////////////////////////////\n",
    "\"\"\"\n",
    "    return a\n",
    "\n",
    "def end_c_file_str() :\n",
    "    a = \"\"\"\n",
    "FILE *file = fopen(\"times.txt\", \"w\");\n",
    "if (file == NULL) {\n",
    "    printf(\"Could not open file for writing.\\\\n\");\n",
    "    return 1;\n",
    "}\n",
    "fprintf(file, times);\n",
    "fclose(file);\n",
    "return 0;\n",
    "}\n",
    "    \"\"\"\n",
    "    return a\n",
    "\n",
    "def compile_c_file(c_file_path):\n",
    "    # gcc is the compiler, -o is used to specify the output file\n",
    "    # 'output' is the name of the output file\n",
    "    command = ['gcc', c_file_path, '-o', 'output']\n",
    "\n",
    "    try:\n",
    "        # Run the command\n",
    "        subprocess.check_call(command)\n",
    "        print(\"Compilation successful.\")\n",
    "    except subprocess.CalledProcessError:\n",
    "        print(\"Compilation failed.\")\n",
    "\n",
    "def run_exe_file(exe_file_path):\n",
    "    try:\n",
    "        # Run the executable file\n",
    "        subprocess.check_call(exe_file_path)\n",
    "        print(\"Execution successful.\")\n",
    "    except subprocess.CalledProcessError:\n",
    "        print(\"Execution failed.\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "datasets = [fetch_ucirepo(id=17), fetch_ucirepo(id=31), fetch_ucirepo(id=545), fetch_ucirepo(id=109), fetch_ucirepo(id=159), fetch_ucirepo(id=94), fetch_ucirepo(id=42), fetch_ucirepo(id=572)]\n",
    "datasets_names = ['Breast Cancer', 'Covertype', 'Rice', 'Wine', 'Magic','Spam', 'Glass', 'Bankruptcy']\n",
    "tree_depths = [3, 5, 10, 15, 20]\n",
    "nr_root_nodes = 20\n",
    "\n",
    "header_file_str = r'const char *dll_paths[] = {'\n",
    "c_file_str = beg_c_file_str(len(datasets), len(tree_depths))\n",
    "dll_file_paths = []\n",
    "nr_results = []\n",
    "nr_features = []\n",
    "nr_instances =[]\n",
    "validation_instances = []\n",
    "validation_instances_str = r''\n",
    "nr_validation_reuses = 1000\n",
    "\n",
    "for i, dataset in enumerate(datasets):\n",
    "    for depth in tree_depths:\n",
    "\n",
    "        # Train models \n",
    "        X = dataset.data.features \n",
    "        y = dataset.data.targets \n",
    "        le = LabelEncoder()\n",
    "        y = le.fit_transform(y)\n",
    "\n",
    "        X.columns = X.columns.str.replace('[', '')\n",
    "        X.columns = X.columns.str.replace(']', '')\n",
    "        X.columns = X.columns.str.replace('<', '')\n",
    "\n",
    "        dtrain = xgb.DMatrix(X, label=y)\n",
    "        params = {\"max_depth\": depth, \"eta\": 0.1, \"objective\": \"multi:softprob\", \"eval_metric\": \"mlogloss\", \"num_class\": len(np.unique(y))}\n",
    "        bst = xgb.train(params, dtrain, num_boost_round=nr_root_nodes, evals=[(dtrain, 'train')])\n",
    "        model = treelite.Model.from_xgboost(bst)\n",
    "\n",
    "        path_no_param = \"./Trees/model\"+str(i)+\"N\"+str(depth)\n",
    "        path_flint = \"./Trees/model\"+str(i)+\"F\"+str(depth)\n",
    "        path_quantize = \"./Trees/model\"+str(i)+\"Q\"+str(depth)\n",
    "\n",
    "\n",
    "        tl2cgen.generate_c_code(model, dirpath=path_no_param, params={})\n",
    "        tl2cgen.generate_c_code(model, dirpath=path_flint, params={\"flint\" : 1})\n",
    "        tl2cgen.generate_c_code(model, dirpath=path_quantize, params={\"quantize\": 1})\n",
    "\n",
    "        # Generate DLLs\n",
    "        path_no_param_dll = path_no_param + '/predict.dll'\n",
    "        path_flint_dll = path_flint + '/predict.dll'\n",
    "        path_quantize_dll = path_quantize + '/predict.dll'\n",
    "\n",
    "        dll_file_paths.append(path_no_param_dll)\n",
    "        dll_file_paths.append(path_flint_dll)\n",
    "        dll_file_paths.append(path_quantize_dll)\n",
    "\n",
    "        FreeTayK(path_quantize + '/main.c')\n",
    "        \n",
    "        # Compile the generated C code into DLLs\n",
    "        subprocess.run(['gcc', '-shared', '-o', path_no_param_dll, path_no_param + '/main.c']),\n",
    "        subprocess.run(['gcc', '-shared', '-o', path_flint_dll, path_flint + '/main.c']),\n",
    "        subprocess.run(['gcc', '-shared', '-o', path_quantize_dll, path_quantize + '/main.c'])\n",
    "\n",
    "    # Generate random validation instances with decimal precision equal to the maximum decimal precision of the training data\n",
    "    min_values_feature = []\n",
    "    max_values_feature = []\n",
    "    nr_decimals = np.zeros((X.shape[0], X.shape[1]))\n",
    "    max_decimals = []\n",
    "\n",
    "    nr_results = len(np.unique(y))\n",
    "    nr_features.append(X.shape[1])\n",
    "    nr_instances.append(X.shape[0])\n",
    "    for k in range(min(X.shape[0],1000)):\n",
    "        for j in range(X.shape[1]):\n",
    "            nr_decimals[i][j] = count_decimals(X.iloc[k, j])\n",
    "\n",
    "    for k in range(X.shape[1]):\n",
    "        max_decimals.append(max(nr_decimals[:, k]))\n",
    "\n",
    "\n",
    "    for k in range(X.shape[1]):\n",
    "        min_values_feature.append(X.iloc[:, k].min())\n",
    "        max_values_feature.append(X.iloc[:, k].max())\n",
    "\n",
    "    validation_instances = np.zeros((nr_instances_validation, X.shape[1]))\n",
    "    for j in range(nr_instances_validation):\n",
    "        for k in range(X.shape[1]):\n",
    "            validation_instances[j][k] = round(random.uniform(min_values_feature[k], max_values_feature[k]), int(max_decimals[k]))\n",
    "    \n",
    "\n",
    "\n",
    "    c_file_str += mid_c_file_str(i, nr_results, nr_validation_reuses)\n",
    "    validation_instances_str += 'float validation_instances'+str(i)+'[][' + str(X.shape[1]) + '] = ' + create_c_array_string(validation_instances) + ';\\n'\n",
    "\n",
    "header_file_str += ', '.join([f'\"{path}\"' for path in dll_file_paths]) + '};\\n'\n",
    "\n",
    "print (header_file_str)\n",
    "header_file_str += validation_instances_str\n",
    "\n",
    "\n",
    "\n",
    "with open('dll_paths.h', 'w') as f:\n",
    "    f.write(header_file_str)\n",
    "\n",
    "\n",
    "c_file_str += end_c_file_str()\n",
    "with open('generated_speedTest.c', 'w') as f:\n",
    "    f.write(c_file_str)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "compile_c_file('generated_speedTest.c')\n",
    "run_exe_file('output')\n",
    "with open('times.txt', 'r') as f:\n",
    "    times_content = f.read()\n",
    "times = np.fromstring(times_content, sep='\\n')\n",
    "times\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read out times.txt without compiling and running the C code again\n",
    "with open('times.txt', 'r') as f:\n",
    "    times_content = f.read()\n",
    "times = np.fromstring(times_content, sep='\\n')\n",
    "times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Figure scaling so that 6 bar groups are 10 wide\n",
    "plt.figure(figsize=((len(times)/3)*(10/6), 5))\n",
    "\n",
    "x = np.arange(0, len(times), 3)\n",
    "\n",
    "width = 2\n",
    "\n",
    "# Create the bar chart for each group with different colors\n",
    "colors = ['red', 'green', 'blue']\n",
    "labels = ['No parameters', 'Flint', 'Quantize']\n",
    "for i in range(3):\n",
    "    plt.bar(x + i*(width/3), times[i::3], color=colors[i], width=width/3, label=labels[i])\n",
    "\n",
    "names = [f\"{name}{depth}\" for name in datasets_names for depth in tree_depths]\n",
    "\n",
    "architecture = platform.architecture()\n",
    "\n",
    "# Set the x-tick labels to the group names and remove the x-axis\n",
    "plt.xticks(x-0.5 + width/2, names)\n",
    "plt.tick_params(axis='x', length=0)\n",
    "\n",
    "plt.ylabel('Seconds')\n",
    "plt.title('Inference times')\n",
    "plt.text(0, -0.1, 'Validation instances per set: '+str(nr_instances_validation)+'\\nValidation set reuses: '+str(nr_validation_reuses)+'\\nNumber of root nodes: '+str(nr_root_nodes)+'\\nSystem info: '+str(architecture), ha='left', va='top', transform=plt.gca().transAxes)\n",
    "\n",
    "# Display the legend\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Figure scaling so that 6 bar groups are 10 wide\n",
    "plt.figure(figsize=((len(times)/(3*len(tree_depths)))*(10/6), 5))\n",
    "\n",
    "speedups_flint = times[0::3]/times[1::3]\n",
    "speedups_quantize = times[0::3]/times[2::3]\n",
    "\n",
    "width = 2\n",
    "speedups = np.ones(len(datasets_names)*3)\n",
    "for i in range(len(datasets_names)):\n",
    "    speedups[i*3+1] = np.mean(speedups_flint[i*len(tree_depths):i*len(tree_depths)+len(tree_depths)])\n",
    "    speedups[i*3+2] = np.mean(speedups_quantize[i*len(tree_depths):i*len(tree_depths)+len(tree_depths)])\n",
    "\n",
    "\n",
    "x = np.arange(0, len(speedups), 3)\n",
    "\n",
    "# Create the bar chart for each group with different colors\n",
    "colors = ['red', 'green', 'blue']\n",
    "labels = ['No parameters', 'Flint', 'Quantize']\n",
    "for i in range(3):\n",
    "    plt.bar(x + i*(width/3), speedups[i::3], color=colors[i], width=width/3, label=labels[i])\n",
    "\n",
    "\n",
    "architecture = platform.architecture()\n",
    "\n",
    "# Set the x-tick labels to the group names and remove the x-axis\n",
    "plt.xticks(x-0.5 + width/2, datasets_names)\n",
    "plt.tick_params(axis='x', length=0)\n",
    "\n",
    "plt.ylabel('Speedup factor')\n",
    "plt.title('Average speedup factors per dataset')\n",
    "plt.text(0, -0.1, 'Validation instances per set: '+str(nr_instances_validation)+'\\nValidation set reuses: '+str(nr_validation_reuses)+'\\nNumber of root nodes: '+str(nr_root_nodes)+'\\nSystem info: '+str(architecture), ha='left', va='top', transform=plt.gca().transAxes)\n",
    "\n",
    "# Display the legend\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "times_no_parameters = [np.mean(times[i*3::len(tree_depths)*3]) for i in range(len(tree_depths))]\n",
    "times_flint = [np.mean(times[i*3+1::len(tree_depths)*3]) for i in range(len(tree_depths))]\n",
    "times_quantize = [np.mean(times[i*3+2::len(tree_depths)*3]) for i in range(len(tree_depths))]\n",
    "\n",
    "architecture = platform.architecture()\n",
    "# Create the line plot for each group with different colors\n",
    "plt.plot(range(len(tree_depths)), times_no_parameters, color='red', marker='*', label='No parameters')\n",
    "plt.plot(range(len(tree_depths)), times_flint, color='green', marker='^', label='Flint')\n",
    "plt.plot(range(len(tree_depths)), times_quantize, color='blue', marker='s', label='Quantize')\n",
    "\n",
    "# Set the x-tick labels to the values in tree_depths\n",
    "plt.xticks(range(len(tree_depths)), tree_depths)\n",
    "\n",
    "plt.xlabel('Maximum tree depth')\n",
    "plt.ylabel('Time [s]')\n",
    "plt.title('Average inference time per tree depth')\n",
    "\n",
    "plt.text(-0.1, -0.15, 'Datasets used:\\n'+str(datasets_names)+'.\\nValidation instances per set: '+str(nr_instances_validation)+'\\nValidation set reuses: '+str(nr_validation_reuses)+'\\nSystem info: '+str(architecture), ha='left', va='top', transform=plt.gca().transAxes)\n",
    "\n",
    "# Display the legend\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "speedups_flint, speedups_quantize = [], []  \n",
    "factors_flint = [times[i*3]/times[i*3+1] for i in range(len(datasets_names)*len(tree_depths))]\n",
    "factors_quantize = [times[i*3]/times[i*3+2] for i in range(len(datasets_names)*len(tree_depths))]\n",
    "\n",
    "for i in range(len(tree_depths)):\n",
    "    speedups_flint.append(np.mean(factors_flint[i::len(tree_depths)]))\n",
    "    speedups_quantize.append(np.mean(factors_quantize[i::len(tree_depths)]))\n",
    "\n",
    "\n",
    "architecture = platform.architecture()\n",
    "# Create the line plot for each group with different colors\n",
    "plt.plot(range(len(tree_depths)), np.ones(len(tree_depths)), color='red', marker='*', label='No parameters')\n",
    "plt.plot(range(len(tree_depths)), speedups_flint, color='green', marker='^', label='Flint')\n",
    "plt.plot(range(len(tree_depths)), speedups_quantize, color='blue', marker='s', label='Quantize')\n",
    "\n",
    "# Set the x-tick labels to the values in tree_depths\n",
    "plt.xticks(range(len(tree_depths)), tree_depths)\n",
    "\n",
    "plt.xlabel('Maximum tree depth')\n",
    "plt.ylabel('Speedup factor')\n",
    "plt.title('Average speedup factor per tree depth')\n",
    "\n",
    "plt.text(-0.1, -0.15, 'Datasets used:\\n'+str(datasets_names)+'.\\nValidation instances per set: '+str(nr_instances_validation)+'\\nValidation set reuses: '+str(nr_validation_reuses)+'\\nSystem info: '+str(architecture), ha='left', va='top', transform=plt.gca().transAxes)\n",
    "\n",
    "# Display the legend\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# WIP\n",
    "times_per_nr_features= []\n",
    "\n",
    "indices = sorted(range(len(nr_features)), key=nr_features.__getitem__)\n",
    "sorted_nr_features = sorted(nr_features)\n",
    "\n",
    "\n",
    "times_no_parameters = [[np.mean(times[i*len(tree_depths)*3:i*len(tree_depths)*3+len(tree_depths)*3:3]) for i in range(len(datasets))][j] for j in indices]\n",
    "times_flint = [[np.mean(times[i*len(tree_depths)*3+1:i*len(tree_depths)*3+len(tree_depths)*3+1:3]) for i in range(len(datasets))][j] for j in indices]\n",
    "times_quantize = [[np.mean(times[i*len(tree_depths)*3+2:i*len(tree_depths)*3+len(tree_depths)*3+2:3]) for i in range(len(datasets))][j] for j in indices]\n",
    "\n",
    "\n",
    "\n",
    "# Create the line plot for each group with different colors\n",
    "plt.plot(range(len(sorted_nr_features)), times_no_parameters, color='red', marker='*', label='No parameters')\n",
    "plt.plot(range(len(sorted_nr_features)), times_flint, color='green', marker='^', label='Flint')\n",
    "plt.plot(range(len(sorted_nr_features)), times_quantize, color='blue', marker='s', label='Quantize')\n",
    "\n",
    "# Set the x-tick labels to the values in tree_depths\n",
    "plt.xticks(range(len(sorted_nr_features)), sorted_nr_features)\n",
    "\n",
    "plt.xlabel('Number of features')\n",
    "plt.ylabel('Time [s]')\n",
    "plt.title('Average inference time per number of features')\n",
    "\n",
    "plt.text(-0.1, -0.15, 'Datasets used: \\n'+str(datasets_names)+'.\\nValidation instances per set: '+str(nr_instances_validation)+'\\nValidation set reuses: '+str(nr_validation_reuses)+'\\nSystem info: '+str(architecture), ha='left', va='top', transform=plt.gca().transAxes)\n",
    "\n",
    "# Display the legend\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
